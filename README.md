ğŸ“Š Machine Learning Datasets

This repository presents a comprehensive experimental comparison of different data preprocessing techniques and machine learning models across multiple datasets.

The project evaluates:

Missing value strategies

Encoding techniques

Label imbalance handling

Scaling methods

Classification models

Regression models

Each dataset is tested systematically, and the best-performing approach is selected based on evaluation metrics.

ğŸ”¹ 1ï¸âƒ£ Missing Value Handling

Different imputation strategies were tested:

Drop rows

Mean

Median

KNN

Forward Fill (ffill)

Backward Fill (bfill)

ğŸ“Œ Best Strategies Per Dataset
Dataset	Best Strategy	Reason
data_1	âœ… Mean	Highest RÂ² + Lowest MSE
data_2	âœ… Median	Best RÂ², MSE, MAE
data_3	âœ… KNN	Clearly best performance margin
data_4	âœ… Drop	Removing noisy rows preserved structure
ğŸ” Key Insight

Mean works well for normally distributed data.

Median is robust to outliers.

KNN captures complex feature relationships.

Dropping rows is effective when imputation distorts data heavily.

ğŸ”¹ 2ï¸âƒ£ Encoding Strategies

Tested:

Ordinal Encoding

One-Hot Encoding

ğŸ“Œ Best Encoding Per Dataset
Dataset	Best Encoding
data_1	âœ… Ordinal
data_2	âœ… One-Hot
ğŸ” Insight

If categorical variables have order â†’ Ordinal works better

If categorical variables are nominal â†’ One-Hot is superior

ğŸ”¹ 3ï¸âƒ£ Label Distribution (Class Imbalance Handling)

Tested:

None

Under-sampling

Over-sampling

SMOTE

Class Weights

ğŸ“Œ Best Strategy Per Dataset
Dataset	Best Strategy
data_1	âœ… None
data_2	âœ… None
data_3	âœ… None
data_4	âœ… Weights
ğŸ” Insight

Data_1,2,3 â†’ Balanced datasets â†’ resampling hurts performance.

Data_4 â†’ Imbalanced â†’ Class weights improved Recall & F1 significantly.

ğŸ”¹ 4ï¸âƒ£ Classification Models

Models Tested:

Logistic Regression

KNN

SVM

Decision Tree

ğŸ“Œ Best Model Per Dataset
Dataset	Best Model
data_4	âœ… Logistic Regression
data_5	âœ… KNN
data_6	âœ… SVM
data_7	âœ… Decision Tree
ğŸ” Insight

Logistic â†’ Best for linear separability

KNN â†’ Works well for clustered data

SVM â†’ Strong for smooth nonlinear boundaries

Decision Tree â†’ Best for rule-based splits

ğŸ”¹ 5ï¸âƒ£ Regression Models

Models Tested:

Linear Regression

KNN Regressor

SVM Regressor

Decision Tree Regressor

ğŸ“Œ Best Regression Model
Dataset	Best Model
data_1	âœ… Linear Regression
data_2	âœ… Decision Tree
data_3	âœ… SVM
ğŸ” Insight

Data_1 â†’ Strong linear relationship

Data_2 â†’ Highly nonlinear (tree-based splits dominate)

Data_3 â†’ Smooth nonlinear boundary (SVM best)

ğŸ”¹ 6ï¸âƒ£ Feature Scaling

Tested:

None

Min-Max Scaling

Standard Scaling

ğŸ“Œ Best Scaling Per Dataset
Dataset	Best Scaling
data_1	âœ… Min-Max (or Standard)
data_2	âœ… None
data_3	âœ… Standard
ğŸ” Insight

Linear & SVM models â†’ Sensitive to scaling

Tree-based models â†’ Scale invariant

Always evaluate scaling impact per dataset

ğŸ“ˆ Key Learnings

âœ” No single preprocessing technique works best for all datasets.
âœ” Data characteristics determine the optimal strategy.
âœ” Model selection must align with dataset structure.
âœ” Preprocessing decisions significantly impact performance.

ğŸ§  Final Summary
Task	Key Finding
Missing Values	Strategy depends on distribution & structure
Encoding	Use ordinal for ordered categories, one-hot for nominal
Imbalance	Use weights only when necessary
Scaling	Required for linear & distance-based models
Classification	Model performance depends on decision boundary shape
Regression	Choose model based on linear vs nonlinear behavior
ğŸš€ Conclusion

This project demonstrates the importance of:

Careful preprocessing

Comparative evaluation

Metric-based model selection

Understanding dataset structure before choosing algorithms